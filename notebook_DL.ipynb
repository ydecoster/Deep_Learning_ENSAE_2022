{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydecoster/Deep_Learning_ENSAE_2022/blob/main/notebook_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "<h1><center><strong><font color=\"chillipepper\">Deep Learning - ENSAE 2022</font></strong></center></h1>\n",
        "<h2><center><strong><font color=\"chillipepper\">Score-based generative modeling through stochastic differential\n",
        "equations</font></strong></center></h2>\n",
        "\n",
        "---\n",
        "\n",
        "Yann DE COSTER (yann.decoster@ensae.fr) et Rémi VIDAL (remi.vidal@ensae.fr)"
      ],
      "metadata": {
        "id": "5VYGeYEWQynl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Au travers de ce notebook nous mettons en application les méthodes de score-based generative modeling développée dans [ce papier](https://arxiv.org/pdf/2011.13456.pdf). Nous appliquons ce type de modèles pour générer de nouveaux échatillons d'images, à la manière des GANs. Les datasets utilisés seront toutefois différents de ceux utilisés dans l'article. Enfin, nous quantifierons nos résultats avec des scores appropriés."
      ],
      "metadata": {
        "id": "wcVsUUi9RiLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports et installations"
      ],
      "metadata": {
        "id": "xxEMKUWzTDdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "uLlqwJFgYwu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install functools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtYPZMfGTK08",
        "outputId": "09c0f911-4cba-4429-d2d8-fedea7601dc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting functools\n",
            "  Downloading functools-0.5.tar.gz (4.9 kB)\n",
            "Building wheels for collected packages: functools\n",
            "  Building wheel for functools (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for functools\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for functools\n",
            "Failed to build functools\n",
            "Installing collected packages: functools\n",
            "    Running setup.py install for functools ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-viqtvs2w/functools_181c1de7d3d843c58c4c943d898de0f0/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-viqtvs2w/functools_181c1de7d3d843c58c4c943d898de0f0/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-1z2qp_gd/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/functools Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import functools\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import tqdm\n",
        "\n",
        "import os\n",
        "import natsort\n",
        "from PIL import *\n",
        "import PIL.Image\n",
        "import helper\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import integrate"
      ],
      "metadata": {
        "id": "ULvBvcMdTDBV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Données"
      ],
      "metadata": {
        "id": "BYTJc3HgYzIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import FashionMNIST"
      ],
      "metadata": {
        "id": "FPm293v5Y0vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après avoir testé les modèles sur la base Fashion MNIST, nous pensions qu'il serait plus intéressant de tester sur une base plus éloignée que MNIST, qui est utilisée dans le notebook des auteurs. Nous avons donc choisi la base de données [Cars](https://ai.stanford.edu/~jkrause/cars/car_dataset.html), qui recense plus de 16 000 images de voitures."
      ],
      "metadata": {
        "id": "7fCzjyxDT-ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -c http://ai.stanford.edu/~jkrause/car196/cars_train.tgz"
      ],
      "metadata": {
        "id": "Ft5-X4D-g3Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af165ade-078d-497e-a61b-d9dbf3676025"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-30 21:17:54--  http://ai.stanford.edu/~jkrause/car196/cars_train.tgz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 979269282 (934M) [application/x-gzip]\n",
            "Saving to: ‘cars_train.tgz’\n",
            "\n",
            "cars_train.tgz      100%[===================>] 933.90M  38.8MB/s    in 26s     \n",
            "\n",
            "2022-04-30 21:18:20 (36.4 MB/s) - ‘cars_train.tgz’ saved [979269282/979269282]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction\n",
        "import tarfile\n",
        "tar = tarfile.open('cars_train.tgz', \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "7wG8fOBlg4H7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataSet(Dataset):\n",
        "    def __init__(self, main_dir, transform):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        all_imgs = os.listdir(main_dir)\n",
        "        self.total_imgs = natsort.natsorted(all_imgs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.total_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        return tensor_image"
      ],
      "metadata": {
        "id": "y3GqEm2Mg8Mm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mise en forme des données\n",
        "\n",
        "data_dir = 'cars_train'\n",
        "batch_size =  32 #@param {'type':'integer'}\n",
        "\n",
        "# Redimensionnement des images, passage en noir et blanc, et création du dataloader\n",
        "transform = transforms.Compose([transforms.Resize([28, 28]),\n",
        "                                transforms.Grayscale(1),\n",
        "                                transforms.ToTensor()])\n",
        "dataset = CustomDataSet(data_dir, transform=transform)\n",
        "data_loader_cars = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "image = next(iter(data_loader_cars))\n",
        "helper.imshow(image[0,:])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SX5muY0thBo0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "45bd26e3-711a-464c-9310-4a48d0de9fb3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0bbe50e9b445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_cars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'helper' has no attribute 'imshow'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition du score-based model"
      ],
      "metadata": {
        "id": "mn38zpfwS4nG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JPzzCnQLQdlN"
      },
      "outputs": [],
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "  \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
        "  def __init__(self, embed_dim, scale=30.):\n",
        "    super().__init__()\n",
        "    # Randomly sample weights during initialization. These weights are fixed \n",
        "    # during optimization and are not trainable.\n",
        "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
        "  def forward(self, x):\n",
        "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
        "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    return self.dense(x)[..., None, None]\n",
        "\n",
        "\n",
        "class ScoreNet(nn.Module):\n",
        "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "  def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
        "    \"\"\"Initialize a time-dependent score-based network.\n",
        "\n",
        "    Args:\n",
        "      marginal_prob_std: A function that takes time t and gives the standard\n",
        "        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
        "      channels: The number of channels for feature maps of each resolution.\n",
        "      embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Gaussian random feature embedding layer for time\n",
        "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
        "         nn.Linear(embed_dim, embed_dim))\n",
        "    # Encoding layers where the resolution decreases\n",
        "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
        "    self.dense1 = Dense(embed_dim, channels[0])\n",
        "    self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
        "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
        "    self.dense2 = Dense(embed_dim, channels[1])\n",
        "    self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense3 = Dense(embed_dim, channels[2])\n",
        "    self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "    self.dense4 = Dense(embed_dim, channels[3])\n",
        "    self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])    \n",
        "\n",
        "    # Decoding layers where the resolution increases\n",
        "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
        "    self.dense5 = Dense(embed_dim, channels[2])\n",
        "    self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)    \n",
        "    self.dense6 = Dense(embed_dim, channels[1])\n",
        "    self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)    \n",
        "    self.dense7 = Dense(embed_dim, channels[0])\n",
        "    self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
        "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
        "    \n",
        "    # The swish activation function\n",
        "    self.act = lambda x: x * torch.sigmoid(x)\n",
        "    self.marginal_prob_std = marginal_prob_std\n",
        "  \n",
        "  def forward(self, x, t): \n",
        "    # Obtain the Gaussian random feature embedding for t   \n",
        "    embed = self.act(self.embed(t))    \n",
        "    # Encoding path\n",
        "    h1 = self.conv1(x)    \n",
        "    ## Incorporate information from t\n",
        "    h1 += self.dense1(embed)\n",
        "    ## Group normalization\n",
        "    h1 = self.gnorm1(h1)\n",
        "    h1 = self.act(h1)\n",
        "    h2 = self.conv2(h1)\n",
        "    h2 += self.dense2(embed)\n",
        "    h2 = self.gnorm2(h2)\n",
        "    h2 = self.act(h2)\n",
        "    h3 = self.conv3(h2)\n",
        "    h3 += self.dense3(embed)\n",
        "    h3 = self.gnorm3(h3)\n",
        "    h3 = self.act(h3)\n",
        "    h4 = self.conv4(h3)\n",
        "    h4 += self.dense4(embed)\n",
        "    h4 = self.gnorm4(h4)\n",
        "    h4 = self.act(h4)\n",
        "\n",
        "    # Decoding path\n",
        "    h = self.tconv4(h4)\n",
        "    ## Skip connection from the encoding path\n",
        "    h += self.dense5(embed)\n",
        "    h = self.tgnorm4(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv3(torch.cat([h, h3], dim=1))\n",
        "    h += self.dense6(embed)\n",
        "    h = self.tgnorm3(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
        "    h += self.dense7(embed)\n",
        "    h = self.tgnorm2(h)\n",
        "    h = self.act(h)\n",
        "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
        "\n",
        "    # Normalize output\n",
        "    h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition de l'équation différentielle stochastique"
      ],
      "metadata": {
        "id": "MrHMw4RDTM7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' #@param ['cuda', 'cpu'] {'type':'string'}\n",
        "\n",
        "def marginal_prob_std(t, sigma):\n",
        "  \"\"\"Compute the mean and standard deviation of $p_{0t}(x(t) | x(0))$.\n",
        "\n",
        "  Args:    \n",
        "    t: A vector of time steps.\n",
        "    sigma: The $\\sigma$ in our SDE.  \n",
        "  \n",
        "  Returns:\n",
        "    The standard deviation.\n",
        "  \"\"\"    \n",
        "  t = torch.tensor(t, device=device)\n",
        "  return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
        "\n",
        "def diffusion_coeff(t, sigma):\n",
        "  \"\"\"Compute the diffusion coefficient of our SDE.\n",
        "\n",
        "  Args:\n",
        "    t: A vector of time steps.\n",
        "    sigma: The $\\sigma$ in our SDE.\n",
        "  \n",
        "  Returns:\n",
        "    The vector of diffusion coefficients.\n",
        "  \"\"\"\n",
        "  return torch.tensor(sigma**t, device=device)\n",
        "  \n",
        "sigma =  25.0#@param {'type':'number'}\n",
        "marginal_prob_std_fn = functools.partial(marginal_prob_std, sigma=sigma)\n",
        "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=sigma)"
      ],
      "metadata": {
        "id": "b7zHVtWfSo5w"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition de la fonction de perte"
      ],
      "metadata": {
        "id": "iyHvYnsjTbvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model, x, marginal_prob_std, eps=1e-5):\n",
        "  \"\"\"The loss function for training score-based generative models.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model instance that represents a \n",
        "      time-dependent score-based model.\n",
        "    x: A mini-batch of training data.    \n",
        "    marginal_prob_std: A function that gives the standard deviation of \n",
        "      the perturbation kernel.\n",
        "    eps: A tolerance value for numerical stability.\n",
        "  \"\"\"\n",
        "  random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps  \n",
        "  z = torch.randn_like(x)\n",
        "  std = marginal_prob_std(random_t)\n",
        "  perturbed_x = x + z * std[:, None, None, None]\n",
        "  score = model(perturbed_x, random_t)\n",
        "  loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1,2,3)))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "pszGzNvvSvys"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w8GxrR2iS232",
        "outputId": "93d79c9c-cc74-45e4-fe60-4dc9e3b8a808"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraînement du modèle"
      ],
      "metadata": {
        "id": "Gjmr_fPsXU2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_model = torch.nn.DataParallel(ScoreNet(marginal_prob_std=marginal_prob_std_fn))\n",
        "score_model = score_model.to(device)\n",
        "\n",
        "# n_epochs =   5#@param {'type':'integer'}\n",
        "# ## size of a mini-batch\n",
        "# batch_size =  32 #@param {'type':'integer'}\n",
        "# ## learning rate\n",
        "# lr=1e-4 #@param {'type':'number'}\n",
        "\n",
        "# dataset = FashionMNIST('.', train=True, transform=transforms.ToTensor(), download=True)\n",
        "# data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# optimizer = Adam(score_model.parameters(), lr=lr)\n",
        "# tqdm_epoch = tqdm.notebook.trange(n_epochs)\n",
        "# for epoch in tqdm_epoch:\n",
        "#   avg_loss = 0.\n",
        "#   num_items = 0\n",
        "#   for x, y in data_loader:\n",
        "#     x = x.to(device)    \n",
        "#     loss = loss_fn(score_model, x, marginal_prob_std_fn)\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()    \n",
        "#     optimizer.step()\n",
        "#     avg_loss += loss.item() * x.shape[0]\n",
        "#     num_items += x.shape[0]\n",
        "#   # Print the averaged training loss so far.\n",
        "#   tqdm_epoch.set_description('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
        "#   # Update the checkpoint after each epoch of training.\n",
        "#   torch.save(score_model.state_dict(), 'ckpt.pth')"
      ],
      "metadata": {
        "id": "fST0md2BSxdw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint de Fashion_MNIST : \n",
        "# !wget \"https://github.com/ydecoster/Deep_Learning_ENSAE_2022/raw/main/ckpt_fashion.pth\"\n",
        "\n",
        "# Checkpoint de Cars : \n",
        "!wget \"https://github.com/ydecoster/Deep_Learning_ENSAE_2022/raw/main/ckpt_cars_300ep_60px.pth\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8nQgKBeTHp8",
        "outputId": "9997bb40-ff7b-44cb-9430-435769aafb0d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-30 21:27:52--  https://github.com/ydecoster/Deep_Learning_ENSAE_2022/raw/main/ckpt_cars_300ep_60px.pth\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ydecoster/Deep_Learning_ENSAE_2022/main/ckpt_cars_300ep_60px.pth [following]\n",
            "--2022-04-30 21:27:53--  https://raw.githubusercontent.com/ydecoster/Deep_Learning_ENSAE_2022/main/ckpt_cars_300ep_60px.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4765613 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘ckpt_cars_300ep_60px.pth’\n",
            "\n",
            "ckpt_cars_300ep_60p 100%[===================>]   4.54M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-04-30 21:27:53 (241 MB/s) - ‘ckpt_cars_300ep_60px.pth’ saved [4765613/4765613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# score_model.load_state_dict(torch.load(\"ckpt_cars_300ep_60px.pth\"))"
      ],
      "metadata": {
        "id": "vtTSj1zGSy08"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition du Euler-Maruyama sampler"
      ],
      "metadata": {
        "id": "Cy8DECv1V65H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The number of sampling steps.\n",
        "num_steps =  100#@param {'type':'integer'}\n",
        "def Euler_Maruyama_sampler(score_model, \n",
        "                           marginal_prob_std,\n",
        "                           diffusion_coeff, \n",
        "                           batch_size=64, \n",
        "                           num_steps=num_steps, \n",
        "                           device='cuda', \n",
        "                           eps=1e-3):\n",
        "  \"\"\"Generate samples from score-based models with the Euler-Maruyama solver.\n",
        "\n",
        "  Args:\n",
        "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
        "    marginal_prob_std: A function that gives the standard deviation of\n",
        "      the perturbation kernel.\n",
        "    diffusion_coeff: A function that gives the diffusion coefficient of the SDE.\n",
        "    batch_size: The number of samplers to generate by calling this function once.\n",
        "    num_steps: The number of sampling steps. \n",
        "      Equivalent to the number of discretized time steps.\n",
        "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
        "    eps: The smallest time step for numerical stability.\n",
        "  \n",
        "  Returns:\n",
        "    Samples.    \n",
        "  \"\"\"\n",
        "  t = torch.ones(batch_size, device=device)\n",
        "  init_x = torch.randn(batch_size, 1, 28, 28, device=device) \\\n",
        "    * marginal_prob_std(t)[:, None, None, None]\n",
        "  time_steps = torch.linspace(1., eps, num_steps, device=device)\n",
        "  step_size = time_steps[0] - time_steps[1]\n",
        "  x = init_x\n",
        "  with torch.no_grad():\n",
        "    for time_step in tqdm.notebook.tqdm(time_steps):      \n",
        "      batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
        "      g = diffusion_coeff(batch_time_step)\n",
        "      mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step) * step_size\n",
        "      x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)      \n",
        "  # Do not include any noise in the last sampling step.\n",
        "  return mean_x"
      ],
      "metadata": {
        "id": "gtsUi7LmT4LT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition du Predictor-Corrector sampler"
      ],
      "metadata": {
        "id": "L0IATj0xVy7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signal_to_noise_ratio = 0.16 #@param {'type':'number'}\n",
        "\n",
        "## The number of sampling steps.\n",
        "num_steps =  100#@param {'type':'integer'}\n",
        "def pc_sampler(score_model, \n",
        "               marginal_prob_std,\n",
        "               diffusion_coeff,\n",
        "               batch_size=64, \n",
        "               num_steps=num_steps, \n",
        "               snr=signal_to_noise_ratio,                \n",
        "               device='cuda',\n",
        "               eps=1e-3):\n",
        "  \"\"\"Generate samples from score-based models with Predictor-Corrector method.\n",
        "\n",
        "  Args:\n",
        "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
        "    marginal_prob_std: A function that gives the standard deviation\n",
        "      of the perturbation kernel.\n",
        "    diffusion_coeff: A function that gives the diffusion coefficient \n",
        "      of the SDE.\n",
        "    batch_size: The number of samplers to generate by calling this function once.\n",
        "    num_steps: The number of sampling steps. \n",
        "      Equivalent to the number of discretized time steps.    \n",
        "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
        "    eps: The smallest time step for numerical stability.\n",
        "  \n",
        "  Returns: \n",
        "    Samples.\n",
        "  \"\"\"\n",
        "  t = torch.ones(batch_size, device=device)\n",
        "  init_x = torch.randn(batch_size, 1, 28, 28, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
        "  time_steps = np.linspace(1., eps, num_steps)\n",
        "  step_size = time_steps[0] - time_steps[1]\n",
        "  x = init_x\n",
        "  with torch.no_grad():\n",
        "    for time_step in tqdm.notebook.tqdm(time_steps):      \n",
        "      batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
        "      # Corrector step (Langevin MCMC)\n",
        "      grad = score_model(x, batch_time_step)\n",
        "      grad_norm = torch.norm(grad.reshape(grad.shape[0], -1), dim=-1).mean()\n",
        "      noise_norm = np.sqrt(np.prod(x.shape[1:]))\n",
        "      langevin_step_size = 2 * (snr * noise_norm / grad_norm)**2\n",
        "      x = x + langevin_step_size * grad + torch.sqrt(2 * langevin_step_size) * torch.randn_like(x)      \n",
        "\n",
        "      # Predictor step (Euler-Maruyama)\n",
        "      g = diffusion_coeff(batch_time_step)\n",
        "      x_mean = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step) * step_size\n",
        "      x = x_mean + torch.sqrt(g**2 * step_size)[:, None, None, None] * torch.randn_like(x)      \n",
        "    \n",
        "    # The last step does not include any noise\n",
        "    return x_mean"
      ],
      "metadata": {
        "id": "sacHekC4T6ok"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition du ODE sampler"
      ],
      "metadata": {
        "id": "n39iymHwVt-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The error tolerance for the black-box ODE solver\n",
        "error_tolerance = 1e-2 #@param {'type': 'number'}\n",
        "def ode_sampler(score_model,\n",
        "                marginal_prob_std,\n",
        "                diffusion_coeff,\n",
        "                batch_size=64, \n",
        "                atol=error_tolerance, \n",
        "                rtol=error_tolerance, \n",
        "                device='cuda', \n",
        "                z=None,\n",
        "                eps=1e-3):\n",
        "  \"\"\"Generate samples from score-based models with black-box ODE solvers.\n",
        "\n",
        "  Args:\n",
        "    score_model: A PyTorch model that represents the time-dependent score-based model.\n",
        "    marginal_prob_std: A function that returns the standard deviation \n",
        "      of the perturbation kernel.\n",
        "    diffusion_coeff: A function that returns the diffusion coefficient of the SDE.\n",
        "    batch_size: The number of samplers to generate by calling this function once.\n",
        "    atol: Tolerance of absolute errors.\n",
        "    rtol: Tolerance of relative errors.\n",
        "    device: 'cuda' for running on GPUs, and 'cpu' for running on CPUs.\n",
        "    z: The latent code that governs the final sample. If None, we start from p_1;\n",
        "      otherwise, we start from the given z.\n",
        "    eps: The smallest time step for numerical stability.\n",
        "  \"\"\"\n",
        "  t = torch.ones(batch_size, device=device)\n",
        "  # Create the latent code\n",
        "  if z is None:\n",
        "    init_x = torch.randn(batch_size, 1, 28, 28, device=device) \\\n",
        "      * marginal_prob_std(t)[:, None, None, None]\n",
        "  else:\n",
        "    init_x = z\n",
        "    \n",
        "  shape = init_x.shape\n",
        "\n",
        "  def score_eval_wrapper(sample, time_steps):\n",
        "    \"\"\"A wrapper of the score-based model for use by the ODE solver.\"\"\"\n",
        "    sample = torch.tensor(sample, device=device, dtype=torch.float32).reshape(shape)\n",
        "    time_steps = torch.tensor(time_steps, device=device, dtype=torch.float32).reshape((sample.shape[0], ))    \n",
        "    with torch.no_grad():    \n",
        "      score = score_model(sample, time_steps)\n",
        "    return score.cpu().numpy().reshape((-1,)).astype(np.float64)\n",
        "  \n",
        "  def ode_func(t, x):        \n",
        "    \"\"\"The ODE function for use by the ODE solver.\"\"\"\n",
        "    time_steps = np.ones((shape[0],)) * t    \n",
        "    g = diffusion_coeff(torch.tensor(t)).cpu().numpy()\n",
        "    return  -0.5 * (g**2) * score_eval_wrapper(x, time_steps)\n",
        "  \n",
        "  # Run the black-box ODE solver.\n",
        "  res = integrate.solve_ivp(ode_func, (1., eps), init_x.reshape(-1).cpu().numpy(), rtol=rtol, atol=atol, method='RK45')  \n",
        "  print(f\"Number of function evaluations: {res.nfev}\")\n",
        "  x = torch.tensor(res.y[:, -1], device=device).reshape(shape)\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "VYifyjYtT8UP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling"
      ],
      "metadata": {
        "id": "-j5uMNzIdEux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "## Load the pre-trained checkpoint from disk.\n",
        "device = 'cuda' #@param ['cuda', 'cpu'] {'type':'string'}\n",
        "\n",
        "# Chargement du checkpoint\n",
        "# ckpt = torch.load('ckpt_fashion.pth', map_location = device)\n",
        "ckpt = torch.load('ckpt_cars_300ep_60px.pth', map_location=device)\n",
        "\n",
        "score_model.load_state_dict(ckpt)\n",
        "\n",
        "sample_batch_size = 64 #@param {'type':'integer'}\n",
        "sampler = ode_sampler #@param ['Euler_Maruyama_sampler', 'pc_sampler', 'ode_sampler'] {'type': 'raw'}\n",
        "\n",
        "## Generate samples using the specified sampler.\n",
        "samples = sampler(score_model, \n",
        "                  marginal_prob_std_fn,\n",
        "                  diffusion_coeff_fn, \n",
        "                  sample_batch_size, \n",
        "                  device=device)\n",
        "\n",
        "## Sample visualization.\n",
        "samples = samples.clamp(0.0, 1.0)\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "sample_grid = make_grid(samples, nrow=int(np.sqrt(sample_batch_size)))\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.axis('off')\n",
        "plt.imshow(sample_grid.permute(1, 2, 0).cpu(), vmin=0., vmax=1.)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "7yz6R7TxT_hn",
        "outputId": "f4d35553-0f06-4d8b-c97b-80f4db97d39b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-8d438db1a6f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ckpt_cars_300ep_60px.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mscore_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msample_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;31m#@param {'type':'integer'}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1498\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tsize mismatch for module.conv1.weight: copying a param with shape torch.Size([64, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 1, 3, 3]).\n\tsize mismatch for module.dense1.dense.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).\n\tsize mismatch for module.dense1.dense.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for module.gnorm1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for module.gnorm1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for module.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3]).\n\tsize mismatch for module.tconv2.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 32, 3, 3]).\n\tsize mismatch for module.dense7.dense.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([32, 256]).\n\tsize mismatch for module.dense7.dense.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for module.tgnorm2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for module.tgnorm2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for module.tconv1.weight: copying a param with shape torch.Size([128, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 1, 3, 3])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ipqbBXwHUAGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}